{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba34ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logreturn</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014110</td>\n",
       "      <td>4.798603</td>\n",
       "      <td>4.291953</td>\n",
       "      <td>0.096115</td>\n",
       "      <td>-0.915052</td>\n",
       "      <td>1.665008</td>\n",
       "      <td>0.334631</td>\n",
       "      <td>-0.286816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004389</td>\n",
       "      <td>4.777986</td>\n",
       "      <td>4.650318</td>\n",
       "      <td>0.538249</td>\n",
       "      <td>-0.841286</td>\n",
       "      <td>1.652642</td>\n",
       "      <td>0.600573</td>\n",
       "      <td>0.067374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.009372</td>\n",
       "      <td>4.764654</td>\n",
       "      <td>4.265499</td>\n",
       "      <td>0.066010</td>\n",
       "      <td>-1.018443</td>\n",
       "      <td>1.649977</td>\n",
       "      <td>0.753251</td>\n",
       "      <td>-0.054899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005736</td>\n",
       "      <td>4.723674</td>\n",
       "      <td>4.242907</td>\n",
       "      <td>0.049920</td>\n",
       "      <td>-1.107658</td>\n",
       "      <td>1.669910</td>\n",
       "      <td>0.458335</td>\n",
       "      <td>-0.511413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011969</td>\n",
       "      <td>4.720298</td>\n",
       "      <td>4.265986</td>\n",
       "      <td>0.088056</td>\n",
       "      <td>-0.870258</td>\n",
       "      <td>1.729706</td>\n",
       "      <td>0.997562</td>\n",
       "      <td>0.209364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   logreturn        f1        f2        f3        f4        f5        f6  \\\n",
       "0   0.014110  4.798603  4.291953  0.096115 -0.915052  1.665008  0.334631   \n",
       "1   0.004389  4.777986  4.650318  0.538249 -0.841286  1.652642  0.600573   \n",
       "2  -0.009372  4.764654  4.265499  0.066010 -1.018443  1.649977  0.753251   \n",
       "3   0.005736  4.723674  4.242907  0.049920 -1.107658  1.669910  0.458335   \n",
       "4   0.011969  4.720298  4.265986  0.088056 -0.870258  1.729706  0.997562   \n",
       "\n",
       "         f7  \n",
       "0 -0.286816  \n",
       "1  0.067374  \n",
       "2 -0.054899  \n",
       "3 -0.511413  \n",
       "4  0.209364  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "file_path = 'pcanewss.xlsx'\n",
    "pca_data = pd.read_excel(file_path)\n",
    "\n",
    "pca_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d8b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pca_data[['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7']]\n",
    "y = pca_data['logreturn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff18723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "val_size = int(len(X_train) * 0.1)\n",
    "X_train, X_val = X_train[:-val_size], X_train[-val_size:]\n",
    "y_train, y_val = y_train[:-val_size], y_train[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78b3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values \n",
    "X_val = X_val.values\n",
    "X_test = X_test.values\n",
    "\n",
    "y_train = y_train.values.reshape(-1, 1) \n",
    "y_val = y_val.values.reshape(-1, 1)\n",
    "y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "X_val_scaled = scaler_x.transform(X_val)\n",
    "X_test_scaled = scaler_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8681679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb0f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66811a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BPNeuralNetwork()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e96933",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 20000 #\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "num_epochs = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ae38a",
   "metadata": {},
   "source": [
    "Due to the instability of the model, we previously applied early stopping with a patience of 10 or 20. However, we ultimately decided to increase the patience and experimented with different learning rates and numbers of epochs. We found that under our chosen learning rate, the model generally converges when the number of epochs reaches 1200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5c6098",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/work/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1924, 1])) that is different to the input size (torch.Size([1924])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/work/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([213, 1])) that is different to the input size (torch.Size([213])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1200], Loss: 0.005560179706662893, Val Loss: 0.04191077873110771\n",
      "Epoch [20/1200], Loss: 0.0017148987390100956, Val Loss: 0.0025222147814929485\n",
      "Epoch [30/1200], Loss: 0.000407375511713326, Val Loss: 0.008438694290816784\n",
      "Epoch [40/1200], Loss: 0.000324101943988353, Val Loss: 0.005788078065961599\n",
      "Epoch [50/1200], Loss: 0.0002788090496324003, Val Loss: 0.0034711314365267754\n",
      "Epoch [60/1200], Loss: 0.00020498175581451505, Val Loss: 0.004968451801687479\n",
      "Epoch [70/1200], Loss: 0.00016823105397634208, Val Loss: 0.003554401220753789\n",
      "Epoch [80/1200], Loss: 0.00015059334691613913, Val Loss: 0.0034991793800145388\n",
      "Epoch [90/1200], Loss: 0.00013892431161366403, Val Loss: 0.003002696903422475\n",
      "Epoch [100/1200], Loss: 0.00013020384358242154, Val Loss: 0.0025182405952364206\n",
      "Epoch [110/1200], Loss: 0.00012338411761447787, Val Loss: 0.002128029242157936\n",
      "Epoch [120/1200], Loss: 0.00011793355224654078, Val Loss: 0.0017689617816358805\n",
      "Epoch [130/1200], Loss: 0.00011332848953315988, Val Loss: 0.0016027148813009262\n",
      "Epoch [140/1200], Loss: 0.00010958804341498762, Val Loss: 0.0014581726863980293\n",
      "Epoch [150/1200], Loss: 0.00010666258458513767, Val Loss: 0.001355624757707119\n",
      "Epoch [160/1200], Loss: 0.00010420336911920458, Val Loss: 0.0012650039279833436\n",
      "Epoch [170/1200], Loss: 0.00010211324843112379, Val Loss: 0.0011994403321295977\n",
      "Epoch [180/1200], Loss: 0.00010031467536464334, Val Loss: 0.001132691279053688\n",
      "Epoch [190/1200], Loss: 9.871061774902046e-05, Val Loss: 0.0010620721150189638\n",
      "Epoch [200/1200], Loss: 9.721518290461972e-05, Val Loss: 0.0009948142105713487\n",
      "Epoch [210/1200], Loss: 9.58173768594861e-05, Val Loss: 0.0009310893365181983\n",
      "Epoch [220/1200], Loss: 9.451255755266175e-05, Val Loss: 0.0008785714744590223\n",
      "Epoch [230/1200], Loss: 9.327312000095844e-05, Val Loss: 0.0008235832210630178\n",
      "Epoch [240/1200], Loss: 9.209821291733533e-05, Val Loss: 0.000768443860579282\n",
      "Epoch [250/1200], Loss: 9.093072003452107e-05, Val Loss: 0.0007226180750876665\n",
      "Epoch [260/1200], Loss: 8.972836803877726e-05, Val Loss: 0.0006690094014629722\n",
      "Epoch [270/1200], Loss: 8.855127089191228e-05, Val Loss: 0.0006333921919576824\n",
      "Epoch [280/1200], Loss: 8.739005716051906e-05, Val Loss: 0.000614838267210871\n",
      "Epoch [290/1200], Loss: 8.630665251985192e-05, Val Loss: 0.000585945846978575\n",
      "Epoch [300/1200], Loss: 8.541045099264011e-05, Val Loss: 0.0005620506126433611\n",
      "Epoch [310/1200], Loss: 8.469326712656766e-05, Val Loss: 0.0005475191865116358\n",
      "Epoch [320/1200], Loss: 8.410480950260535e-05, Val Loss: 0.0005179618601687253\n",
      "Epoch [330/1200], Loss: 8.363092638319358e-05, Val Loss: 0.0004800829046871513\n",
      "Epoch [340/1200], Loss: 8.32576333777979e-05, Val Loss: 0.00044835577136836946\n",
      "Epoch [350/1200], Loss: 8.295433508465067e-05, Val Loss: 0.0004218744579702616\n",
      "Epoch [360/1200], Loss: 8.270712714875117e-05, Val Loss: 0.0003986416559200734\n",
      "Epoch [370/1200], Loss: 8.249773236457258e-05, Val Loss: 0.00037649524165317416\n",
      "Epoch [380/1200], Loss: 8.232058462454006e-05, Val Loss: 0.0003547545347828418\n",
      "Epoch [390/1200], Loss: 8.216859714593738e-05, Val Loss: 0.0003345986770000309\n",
      "Epoch [400/1200], Loss: 8.203326433431357e-05, Val Loss: 0.0003157429164275527\n",
      "Epoch [410/1200], Loss: 8.190987864509225e-05, Val Loss: 0.00029812054708600044\n",
      "Epoch [420/1200], Loss: 8.179515862138942e-05, Val Loss: 0.0002815790649037808\n",
      "Epoch [430/1200], Loss: 8.168919157469645e-05, Val Loss: 0.0002658011217135936\n",
      "Epoch [440/1200], Loss: 8.159012213582173e-05, Val Loss: 0.00025144091341644526\n",
      "Epoch [450/1200], Loss: 8.149637869792059e-05, Val Loss: 0.00023847857664804906\n",
      "Epoch [460/1200], Loss: 8.140447607729584e-05, Val Loss: 0.00022612376778852195\n",
      "Epoch [470/1200], Loss: 8.13176084193401e-05, Val Loss: 0.00021484121680259705\n",
      "Epoch [480/1200], Loss: 8.123511361191049e-05, Val Loss: 0.00020532074267975986\n",
      "Epoch [490/1200], Loss: 8.115921809803694e-05, Val Loss: 0.00019717105897143483\n",
      "Epoch [500/1200], Loss: 8.108899783110246e-05, Val Loss: 0.00018998539599124342\n",
      "Epoch [510/1200], Loss: 8.102514402708039e-05, Val Loss: 0.00018379806715529412\n",
      "Epoch [520/1200], Loss: 8.096700184978545e-05, Val Loss: 0.0001784407504601404\n",
      "Epoch [530/1200], Loss: 8.091240306384861e-05, Val Loss: 0.00017383387603331357\n",
      "Epoch [540/1200], Loss: 8.086238813120872e-05, Val Loss: 0.0001699529675533995\n",
      "Epoch [550/1200], Loss: 8.08152835816145e-05, Val Loss: 0.0001667292817728594\n",
      "Epoch [560/1200], Loss: 8.077149686869234e-05, Val Loss: 0.00016401080938521773\n",
      "Epoch [570/1200], Loss: 8.072918717516586e-05, Val Loss: 0.00016171237803064287\n",
      "Epoch [580/1200], Loss: 8.069009345490485e-05, Val Loss: 0.00015967627405188978\n",
      "Epoch [590/1200], Loss: 8.065485599217936e-05, Val Loss: 0.00015796547813806683\n",
      "Epoch [600/1200], Loss: 8.062335109570995e-05, Val Loss: 0.00015663648082409054\n",
      "Epoch [610/1200], Loss: 8.059337415033951e-05, Val Loss: 0.0001555975468363613\n",
      "Epoch [620/1200], Loss: 8.056549995671958e-05, Val Loss: 0.0001547769788885489\n",
      "Epoch [630/1200], Loss: 8.053902274696156e-05, Val Loss: 0.0001541359961265698\n",
      "Epoch [640/1200], Loss: 8.051432814681903e-05, Val Loss: 0.00015357040683738887\n",
      "Epoch [650/1200], Loss: 8.04918454377912e-05, Val Loss: 0.00015314799384213984\n",
      "Epoch [660/1200], Loss: 8.047118899412453e-05, Val Loss: 0.00015279414947144687\n",
      "Epoch [670/1200], Loss: 8.04514202172868e-05, Val Loss: 0.0001525553670944646\n",
      "Epoch [680/1200], Loss: 8.043411799008027e-05, Val Loss: 0.00015241945220623165\n",
      "Epoch [690/1200], Loss: 8.041846012929454e-05, Val Loss: 0.0001523853570688516\n",
      "Epoch [700/1200], Loss: 8.040430839173496e-05, Val Loss: 0.00015242992958519608\n",
      "Epoch [710/1200], Loss: 8.039101521717384e-05, Val Loss: 0.00015249554417096078\n",
      "Epoch [720/1200], Loss: 8.03772927611135e-05, Val Loss: 0.0001525375701021403\n",
      "Epoch [730/1200], Loss: 8.036317012738436e-05, Val Loss: 0.00015262294618878514\n",
      "Epoch [740/1200], Loss: 8.035003702389076e-05, Val Loss: 0.00015276084013748914\n",
      "Epoch [750/1200], Loss: 8.033779158722609e-05, Val Loss: 0.0001529509900137782\n",
      "Epoch [760/1200], Loss: 8.032626647036523e-05, Val Loss: 0.00015313354379031807\n",
      "Epoch [770/1200], Loss: 8.03150178398937e-05, Val Loss: 0.0001532882743049413\n",
      "Epoch [780/1200], Loss: 8.030439494177699e-05, Val Loss: 0.00015342589176725596\n",
      "Epoch [790/1200], Loss: 8.029483433347195e-05, Val Loss: 0.0001535378396511078\n",
      "Epoch [800/1200], Loss: 8.028570300666615e-05, Val Loss: 0.00015364462160505354\n",
      "Epoch [810/1200], Loss: 8.02766444394365e-05, Val Loss: 0.00015374261420220137\n",
      "Epoch [820/1200], Loss: 8.026776049518958e-05, Val Loss: 0.00015380102558992803\n",
      "Epoch [830/1200], Loss: 8.025932038435712e-05, Val Loss: 0.00015382218407467008\n",
      "Epoch [840/1200], Loss: 8.025122224353254e-05, Val Loss: 0.00015382577839773148\n",
      "Epoch [850/1200], Loss: 8.024372800718993e-05, Val Loss: 0.000153865126776509\n",
      "Epoch [860/1200], Loss: 8.023586997296661e-05, Val Loss: 0.00015391057240776718\n",
      "Epoch [870/1200], Loss: 8.022773545235395e-05, Val Loss: 0.00015392644854728132\n",
      "Epoch [880/1200], Loss: 8.021992834983394e-05, Val Loss: 0.00015392109344247729\n",
      "Epoch [890/1200], Loss: 8.02121139713563e-05, Val Loss: 0.00015391357010230422\n",
      "Epoch [900/1200], Loss: 8.020442328415811e-05, Val Loss: 0.00015387857274618\n",
      "Epoch [910/1200], Loss: 8.01968271844089e-05, Val Loss: 0.0001538273791084066\n",
      "Epoch [920/1200], Loss: 8.018893277039751e-05, Val Loss: 0.0001538024953333661\n",
      "Epoch [930/1200], Loss: 8.018122025532648e-05, Val Loss: 0.00015376764349639416\n",
      "Epoch [940/1200], Loss: 8.017349318834022e-05, Val Loss: 0.00015371003246400505\n",
      "Epoch [950/1200], Loss: 8.01658388809301e-05, Val Loss: 0.00015365181025117636\n",
      "Epoch [960/1200], Loss: 8.015810453798622e-05, Val Loss: 0.00015354760398622602\n",
      "Epoch [970/1200], Loss: 8.015017374418676e-05, Val Loss: 0.0001533942122478038\n",
      "Epoch [980/1200], Loss: 8.014308696147054e-05, Val Loss: 0.00015312789764720947\n",
      "Epoch [990/1200], Loss: 8.013619662960991e-05, Val Loss: 0.0001528051943751052\n",
      "Epoch [1000/1200], Loss: 8.012901525944471e-05, Val Loss: 0.00015246904513332993\n",
      "Epoch [1010/1200], Loss: 8.012178295757622e-05, Val Loss: 0.0001520895166322589\n",
      "Epoch [1020/1200], Loss: 8.011448517208919e-05, Val Loss: 0.00015165319200605154\n",
      "Epoch [1030/1200], Loss: 8.010747842490673e-05, Val Loss: 0.00015117268776521087\n",
      "Epoch [1040/1200], Loss: 8.01008427515626e-05, Val Loss: 0.00015068190987221897\n",
      "Epoch [1050/1200], Loss: 8.009465091163293e-05, Val Loss: 0.0001501963270129636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1060/1200], Loss: 8.008881559362635e-05, Val Loss: 0.00014971510972827673\n",
      "Epoch [1070/1200], Loss: 8.00832494860515e-05, Val Loss: 0.0001492217561462894\n",
      "Epoch [1080/1200], Loss: 8.007803262444213e-05, Val Loss: 0.00014872648171149194\n",
      "Epoch [1090/1200], Loss: 8.007309952517971e-05, Val Loss: 0.0001482562511228025\n",
      "Epoch [1100/1200], Loss: 8.00684210844338e-05, Val Loss: 0.00014780017954763025\n",
      "Epoch [1110/1200], Loss: 8.006417192518711e-05, Val Loss: 0.00014736410230398178\n",
      "Epoch [1120/1200], Loss: 8.006020652828738e-05, Val Loss: 0.00014694598212372512\n",
      "Epoch [1130/1200], Loss: 8.005636482266709e-05, Val Loss: 0.00014655340055469424\n",
      "Epoch [1140/1200], Loss: 8.005272684386e-05, Val Loss: 0.0001462092186557129\n",
      "Epoch [1150/1200], Loss: 8.004916890058666e-05, Val Loss: 0.0001459022460039705\n",
      "Epoch [1160/1200], Loss: 8.004571282071993e-05, Val Loss: 0.00014561980788130313\n",
      "Epoch [1170/1200], Loss: 8.004235860425979e-05, Val Loss: 0.00014535561786033213\n",
      "Epoch [1180/1200], Loss: 8.003907714737579e-05, Val Loss: 0.00014511415793094784\n",
      "Epoch [1190/1200], Loss: 8.003573748283088e-05, Val Loss: 0.00014489654859062284\n",
      "Epoch [1200/1200], Loss: 8.003242692211643e-05, Val Loss: 0.0001447024114895612\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs.flatten(), y_train_tensor)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs.flatten(), y_val_tensor)\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss <= best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121d73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE (original scale): 6.213107553776354e-05\n",
      "Correlation between y_test and y_pred: 0.03058755727879976\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor).flatten() \n",
    "    test_loss = criterion(test_outputs, y_test_tensor.flatten()) \n",
    "\n",
    "    y_pred = test_outputs.numpy()\n",
    "    y_test_array = y_test_tensor.numpy()\n",
    "\n",
    "    test_mse = mean_squared_error(y_test_array, y_pred)\n",
    "\n",
    "    print(f\"Test MSE (original scale): {test_mse}\")\n",
    "\n",
    "correlation = np.corrcoef(y_test_array.flatten(), y_pred.flatten())[0, 1]\n",
    "print(f\"Correlation between y_test and y_pred: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8350cc",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40357f00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/60], Loss: -0.0484, Val Loss: -0.0710, Train Correlation: 0.0484, Val Correlation: 0.0710\n",
      "Epoch [20/60], Loss: -0.0546, Val Loss: -0.0728, Train Correlation: 0.0546, Val Correlation: 0.0728\n",
      "Epoch [30/60], Loss: -0.0608, Val Loss: -0.0745, Train Correlation: 0.0608, Val Correlation: 0.0745\n",
      "Epoch [40/60], Loss: -0.0672, Val Loss: -0.0763, Train Correlation: 0.0672, Val Correlation: 0.0763\n",
      "Epoch [50/60], Loss: -0.0737, Val Loss: -0.0780, Train Correlation: 0.0737, Val Correlation: 0.0780\n",
      "Epoch [60/60], Loss: -0.0800, Val Loss: -0.0793, Train Correlation: 0.0800, Val Correlation: 0.0793\n",
      "Test Loss (Negative Correlation): 0.009302489459514618\n",
      "Correlation between y_test and y_pred: -0.009302484298110007\n"
     ]
    }
   ],
   "source": [
    "model = BPNeuralNetwork()\n",
    "\n",
    "def negative_correlation_loss(output, target):\n",
    "\n",
    "    output = output - torch.mean(output)  \n",
    "    target = target - torch.mean(target)  \n",
    "    numerator = torch.sum(output * target) \n",
    "    denominator = torch.sqrt(torch.sum(output ** 2)) * torch.sqrt(torch.sum(target ** 2))  \n",
    "    correlation = numerator / (denominator + 1e-8)  \n",
    "    return -correlation \n",
    "\n",
    "criterion = negative_correlation_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "patience = 10000  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "num_epochs = 60\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    outputs = model(X_train_tensor).flatten()\n",
    "    loss = criterion(outputs, y_train_tensor.flatten())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor).flatten()\n",
    "        val_loss = criterion(val_outputs, y_val_tensor.flatten())\n",
    "\n",
    "    # Calculate correlation for training\n",
    "    train_correlation = np.corrcoef(\n",
    "        y_train_tensor.numpy().flatten(), outputs.detach().numpy().flatten()\n",
    "    )[0, 1]\n",
    "\n",
    "    # Calculate correlation for validation\n",
    "    val_correlation = np.corrcoef(\n",
    "        y_val_tensor.numpy().flatten(), val_outputs.numpy().flatten()\n",
    "    )[0, 1]\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss <= best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    # Print metrics every 10 epochs or at the last epoch\n",
    "    if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "            f\"Train Correlation: {train_correlation:.4f}, Val Correlation: {val_correlation:.4f}\"\n",
    "        )\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor).flatten()\n",
    "    test_loss = criterion(test_outputs, y_test_tensor.flatten()).item()\n",
    "\n",
    "    y_pred = test_outputs.numpy()\n",
    "    y_test_array = y_test_tensor.numpy()\n",
    "\n",
    "    test_correlation = np.corrcoef(y_test_array.flatten(), y_pred.flatten())[0, 1]\n",
    "\n",
    "    print(f\"Test Loss (Negative Correlation): {test_loss}\")\n",
    "    print(f\"Correlation between y_test and y_pred: {test_correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b5f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3492563f",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fb98032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 7.965149397262919e-05\n",
      "Validation MSE: 0.00014415698439906142\n",
      "Test MSE: 6.294942001220667e-05\n",
      "Correlation between y_test and y_test_pred: 0.04226254917424558\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_train_pred = lr.predict(X_train_scaled)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "\n",
    "y_val_pred = lr.predict(X_val_scaled)\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "print(f\"Validation MSE: {val_mse}\")\n",
    "\n",
    "y_test_pred = lr.predict(X_test_scaled)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "\n",
    "correlation = np.corrcoef(y_test.flatten(), y_test_pred.flatten())[0, 1]\n",
    "print(f\"Correlation between y_test and y_test_pred: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb722f",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39a0851e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b32c4740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_naive = np.array([0] * 535).reshape(535, 1)\n",
    "y_test_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e76a8fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.158865164661923e-05"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_test_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b0237",
   "metadata": {},
   "source": [
    "# Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c06f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_path = 'projectdata.xlsx'\n",
    "data = pd.read_excel(new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca921aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>logreturn</th>\n",
       "      <th>HS300</th>\n",
       "      <th>CompriceIndex</th>\n",
       "      <th>PE</th>\n",
       "      <th>S&amp;P500Volatility3</th>\n",
       "      <th>deprate</th>\n",
       "      <th>deprate3</th>\n",
       "      <th>deprate6</th>\n",
       "      <th>...</th>\n",
       "      <th>setpriceCOMEXsilver</th>\n",
       "      <th>Londonspotgold</th>\n",
       "      <th>Londonspotsilver</th>\n",
       "      <th>SHgoldspotvolumeAu9999</th>\n",
       "      <th>SHgoldspotClspriceAu9999</th>\n",
       "      <th>Goldproduction</th>\n",
       "      <th>Goldcondemand</th>\n",
       "      <th>USGoldcondemand</th>\n",
       "      <th>tradevolume</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>331.95</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>2276.385</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.9824</td>\n",
       "      <td>24.70</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>29.296</td>\n",
       "      <td>1032.546</td>\n",
       "      <td>28.92</td>\n",
       "      <td>7869.6</td>\n",
       "      <td>331.98</td>\n",
       "      <td>19.733</td>\n",
       "      <td>259.79</td>\n",
       "      <td>14.05</td>\n",
       "      <td>82316</td>\n",
       "      <td>99406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>333.41</td>\n",
       "      <td>0.004389</td>\n",
       "      <td>2290.601</td>\n",
       "      <td>155.28</td>\n",
       "      <td>13.9454</td>\n",
       "      <td>24.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>28.683</td>\n",
       "      <td>1049.880</td>\n",
       "      <td>29.40</td>\n",
       "      <td>9875.8</td>\n",
       "      <td>332.58</td>\n",
       "      <td>19.733</td>\n",
       "      <td>259.79</td>\n",
       "      <td>14.05</td>\n",
       "      <td>95518</td>\n",
       "      <td>91782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-09</td>\n",
       "      <td>330.30</td>\n",
       "      <td>-0.009372</td>\n",
       "      <td>2368.570</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.9753</td>\n",
       "      <td>24.12</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>28.782</td>\n",
       "      <td>1045.443</td>\n",
       "      <td>28.85</td>\n",
       "      <td>9369.4</td>\n",
       "      <td>331.04</td>\n",
       "      <td>19.733</td>\n",
       "      <td>259.79</td>\n",
       "      <td>14.05</td>\n",
       "      <td>69482</td>\n",
       "      <td>90598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>332.20</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>2447.349</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.1068</td>\n",
       "      <td>23.68</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>29.815</td>\n",
       "      <td>1056.947</td>\n",
       "      <td>29.69</td>\n",
       "      <td>8523.4</td>\n",
       "      <td>332.05</td>\n",
       "      <td>19.733</td>\n",
       "      <td>259.79</td>\n",
       "      <td>14.05</td>\n",
       "      <td>63840</td>\n",
       "      <td>86898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-11</td>\n",
       "      <td>336.20</td>\n",
       "      <td>0.011969</td>\n",
       "      <td>2435.608</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.1205</td>\n",
       "      <td>24.11</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>29.890</td>\n",
       "      <td>1064.405</td>\n",
       "      <td>29.81</td>\n",
       "      <td>5768.8</td>\n",
       "      <td>335.50</td>\n",
       "      <td>19.733</td>\n",
       "      <td>259.79</td>\n",
       "      <td>14.05</td>\n",
       "      <td>61830</td>\n",
       "      <td>85756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   price  logreturn     HS300  CompriceIndex       PE  \\\n",
       "0 2012-01-05  331.95   0.014110  2276.385           0.00  13.9824   \n",
       "1 2012-01-06  333.41   0.004389  2290.601         155.28  13.9454   \n",
       "2 2012-01-09  330.30  -0.009372  2368.570           0.00  13.9753   \n",
       "3 2012-01-10  332.20   0.005736  2447.349           0.00  14.1068   \n",
       "4 2012-01-11  336.20   0.011969  2435.608           0.00  14.1205   \n",
       "\n",
       "   S&P500Volatility3  deprate  deprate3  deprate6  ...  setpriceCOMEXsilver  \\\n",
       "0              24.70      0.5       3.1       3.3  ...               29.296   \n",
       "1              24.09      0.5       3.1       3.3  ...               28.683   \n",
       "2              24.12      0.5       3.1       3.3  ...               28.782   \n",
       "3              23.68      0.5       3.1       3.3  ...               29.815   \n",
       "4              24.11      0.5       3.1       3.3  ...               29.890   \n",
       "\n",
       "   Londonspotgold  Londonspotsilver  SHgoldspotvolumeAu9999  \\\n",
       "0        1032.546             28.92                  7869.6   \n",
       "1        1049.880             29.40                  9875.8   \n",
       "2        1045.443             28.85                  9369.4   \n",
       "3        1056.947             29.69                  8523.4   \n",
       "4        1064.405             29.81                  5768.8   \n",
       "\n",
       "   SHgoldspotClspriceAu9999  Goldproduction  Goldcondemand  USGoldcondemand  \\\n",
       "0                    331.98          19.733         259.79            14.05   \n",
       "1                    332.58          19.733         259.79            14.05   \n",
       "2                    331.04          19.733         259.79            14.05   \n",
       "3                    332.05          19.733         259.79            14.05   \n",
       "4                    335.50          19.733         259.79            14.05   \n",
       "\n",
       "   tradevolume  position  \n",
       "0        82316     99406  \n",
       "1        95518     91782  \n",
       "2        69482     90598  \n",
       "3        63840     86898  \n",
       "4        61830     85756  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c977985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    331.95\n",
       "1    333.41\n",
       "2    330.30\n",
       "3    332.20\n",
       "4    336.20\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['price'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec452b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_y = data['logreturn']\n",
    "original_X = data.drop(columns=['date', 'price', 'logreturn']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62c335da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2672, 29)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c94008a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_size = int(len(original_X) * 0.8)\n",
    "original_X_train, original_X_test = X[:original_train_size], X[original_train_size:]\n",
    "original_y_train, original_y_test = y[:original_train_size], y[original_train_size:]\n",
    "\n",
    "val_size = int(len(original_X_train) * 0.1)\n",
    "original_X_train, original_X_val = original_X_train[:-val_size], original_X_train[-val_size:]\n",
    "original_y_train, original_y_val = original_y_train[:-val_size], original_y_train[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a42ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_X_train = original_X_train.values \n",
    "original_X_val = original_X_val.values\n",
    "original_X_test = original_X_test.values\n",
    "\n",
    "original_y_train = original_y_train.values.reshape(-1, 1) \n",
    "original_y_val = original_y_val.values.reshape(-1, 1)\n",
    "original_y_test = original_y_test.values.reshape(-1, 1)\n",
    "\n",
    "original_scaler_x = StandardScaler()\n",
    "\n",
    "original_X_train_scaled = original_scaler_x.fit_transform(original_X_train)\n",
    "original_X_val_scaled = original_scaler_x.transform(original_X_val)\n",
    "original_X_test_scaled = original_scaler_x.transform(original_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16bfbed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_X_train_tensor = torch.tensor(original_X_train_scaled, dtype=torch.float32)\n",
    "original_y_train_tensor = torch.tensor(original_y_train, dtype=torch.float32)\n",
    "\n",
    "original_X_val_tensor = torch.tensor(original_X_val_scaled, dtype=torch.float32)\n",
    "original_y_val_tensor = torch.tensor(original_y_val, dtype=torch.float32)\n",
    "\n",
    "original_X_test_tensor = torch.tensor(original_X_test_scaled, dtype=torch.float32)\n",
    "original_y_test_tensor = torch.tensor(original_y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "deeddea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: -0.0337, Val Loss: 0.0420, Train Correlation: 0.0337, Val Correlation: -0.0420\n",
      "Epoch [20/1000], Loss: -0.0531, Val Loss: 0.0338, Train Correlation: 0.0531, Val Correlation: -0.0338\n",
      "Epoch [30/1000], Loss: -0.0723, Val Loss: 0.0257, Train Correlation: 0.0723, Val Correlation: -0.0257\n",
      "Epoch [40/1000], Loss: -0.0933, Val Loss: 0.0178, Train Correlation: 0.0933, Val Correlation: -0.0178\n",
      "Epoch [50/1000], Loss: -0.1185, Val Loss: 0.0111, Train Correlation: 0.1185, Val Correlation: -0.0111\n",
      "Epoch [60/1000], Loss: -0.1504, Val Loss: 0.0010, Train Correlation: 0.1504, Val Correlation: -0.0010\n",
      "Epoch [70/1000], Loss: -0.1830, Val Loss: -0.0102, Train Correlation: 0.1830, Val Correlation: 0.0102\n",
      "Epoch [80/1000], Loss: -0.2112, Val Loss: -0.0135, Train Correlation: 0.2112, Val Correlation: 0.0135\n",
      "Epoch [90/1000], Loss: -0.2374, Val Loss: -0.0116, Train Correlation: 0.2374, Val Correlation: 0.0116\n",
      "Epoch [100/1000], Loss: -0.2611, Val Loss: -0.0069, Train Correlation: 0.2611, Val Correlation: 0.0069\n",
      "Epoch [110/1000], Loss: -0.2832, Val Loss: -0.0004, Train Correlation: 0.2832, Val Correlation: 0.0004\n",
      "Epoch [120/1000], Loss: -0.3042, Val Loss: 0.0060, Train Correlation: 0.3042, Val Correlation: -0.0060\n",
      "Epoch [130/1000], Loss: -0.3247, Val Loss: 0.0127, Train Correlation: 0.3247, Val Correlation: -0.0127\n",
      "Epoch [140/1000], Loss: -0.3445, Val Loss: 0.0191, Train Correlation: 0.3445, Val Correlation: -0.0191\n",
      "Epoch [150/1000], Loss: -0.3646, Val Loss: 0.0251, Train Correlation: 0.3646, Val Correlation: -0.0251\n",
      "Epoch [160/1000], Loss: -0.3841, Val Loss: 0.0301, Train Correlation: 0.3841, Val Correlation: -0.0301\n",
      "Epoch [170/1000], Loss: -0.4040, Val Loss: 0.0341, Train Correlation: 0.4040, Val Correlation: -0.0341\n",
      "Epoch [180/1000], Loss: -0.4248, Val Loss: 0.0371, Train Correlation: 0.4248, Val Correlation: -0.0371\n",
      "Epoch [190/1000], Loss: -0.4450, Val Loss: 0.0407, Train Correlation: 0.4450, Val Correlation: -0.0407\n",
      "Epoch [200/1000], Loss: -0.4648, Val Loss: 0.0447, Train Correlation: 0.4648, Val Correlation: -0.0447\n",
      "Epoch [210/1000], Loss: -0.4839, Val Loss: 0.0514, Train Correlation: 0.4839, Val Correlation: -0.0514\n",
      "Epoch [220/1000], Loss: -0.5028, Val Loss: 0.0595, Train Correlation: 0.5028, Val Correlation: -0.0595\n",
      "Epoch [230/1000], Loss: -0.5211, Val Loss: 0.0650, Train Correlation: 0.5211, Val Correlation: -0.0650\n",
      "Epoch [240/1000], Loss: -0.5390, Val Loss: 0.0697, Train Correlation: 0.5390, Val Correlation: -0.0697\n",
      "Epoch [250/1000], Loss: -0.5560, Val Loss: 0.0723, Train Correlation: 0.5560, Val Correlation: -0.0723\n",
      "Epoch [260/1000], Loss: -0.5720, Val Loss: 0.0733, Train Correlation: 0.5720, Val Correlation: -0.0733\n",
      "Epoch [270/1000], Loss: -0.5876, Val Loss: 0.0740, Train Correlation: 0.5876, Val Correlation: -0.0740\n",
      "Epoch [280/1000], Loss: -0.6023, Val Loss: 0.0745, Train Correlation: 0.6023, Val Correlation: -0.0745\n",
      "Epoch [290/1000], Loss: -0.6162, Val Loss: 0.0749, Train Correlation: 0.6162, Val Correlation: -0.0749\n",
      "Epoch [300/1000], Loss: -0.6290, Val Loss: 0.0762, Train Correlation: 0.6290, Val Correlation: -0.0762\n",
      "Epoch [310/1000], Loss: -0.6412, Val Loss: 0.0777, Train Correlation: 0.6412, Val Correlation: -0.0777\n",
      "Epoch [320/1000], Loss: -0.6528, Val Loss: 0.0796, Train Correlation: 0.6528, Val Correlation: -0.0796\n",
      "Epoch [330/1000], Loss: -0.6640, Val Loss: 0.0817, Train Correlation: 0.6640, Val Correlation: -0.0817\n",
      "Epoch [340/1000], Loss: -0.6747, Val Loss: 0.0847, Train Correlation: 0.6747, Val Correlation: -0.0847\n",
      "Epoch [350/1000], Loss: -0.6847, Val Loss: 0.0876, Train Correlation: 0.6847, Val Correlation: -0.0876\n",
      "Epoch [360/1000], Loss: -0.6942, Val Loss: 0.0901, Train Correlation: 0.6942, Val Correlation: -0.0901\n",
      "Epoch [370/1000], Loss: -0.7031, Val Loss: 0.0917, Train Correlation: 0.7031, Val Correlation: -0.0917\n",
      "Epoch [380/1000], Loss: -0.7116, Val Loss: 0.0927, Train Correlation: 0.7116, Val Correlation: -0.0927\n",
      "Epoch [390/1000], Loss: -0.7195, Val Loss: 0.0934, Train Correlation: 0.7195, Val Correlation: -0.0934\n",
      "Epoch [400/1000], Loss: -0.7272, Val Loss: 0.0938, Train Correlation: 0.7272, Val Correlation: -0.0938\n",
      "Epoch [410/1000], Loss: -0.7345, Val Loss: 0.0951, Train Correlation: 0.7345, Val Correlation: -0.0951\n",
      "Epoch [420/1000], Loss: -0.7417, Val Loss: 0.0973, Train Correlation: 0.7417, Val Correlation: -0.0973\n",
      "Epoch [430/1000], Loss: -0.7487, Val Loss: 0.0995, Train Correlation: 0.7487, Val Correlation: -0.0995\n",
      "Epoch [440/1000], Loss: -0.7555, Val Loss: 0.1015, Train Correlation: 0.7555, Val Correlation: -0.1015\n",
      "Epoch [450/1000], Loss: -0.7621, Val Loss: 0.1033, Train Correlation: 0.7621, Val Correlation: -0.1033\n",
      "Epoch [460/1000], Loss: -0.7684, Val Loss: 0.1046, Train Correlation: 0.7684, Val Correlation: -0.1046\n",
      "Epoch [470/1000], Loss: -0.7745, Val Loss: 0.1060, Train Correlation: 0.7745, Val Correlation: -0.1060\n",
      "Epoch [480/1000], Loss: -0.7807, Val Loss: 0.1083, Train Correlation: 0.7807, Val Correlation: -0.1083\n",
      "Epoch [490/1000], Loss: -0.7867, Val Loss: 0.1109, Train Correlation: 0.7867, Val Correlation: -0.1109\n",
      "Epoch [500/1000], Loss: -0.7923, Val Loss: 0.1130, Train Correlation: 0.7923, Val Correlation: -0.1130\n",
      "Epoch [510/1000], Loss: -0.7974, Val Loss: 0.1147, Train Correlation: 0.7974, Val Correlation: -0.1147\n",
      "Epoch [520/1000], Loss: -0.8026, Val Loss: 0.1161, Train Correlation: 0.8026, Val Correlation: -0.1161\n",
      "Epoch [530/1000], Loss: -0.8076, Val Loss: 0.1181, Train Correlation: 0.8076, Val Correlation: -0.1181\n",
      "Epoch [540/1000], Loss: -0.8120, Val Loss: 0.1193, Train Correlation: 0.8120, Val Correlation: -0.1193\n",
      "Epoch [550/1000], Loss: -0.8163, Val Loss: 0.1203, Train Correlation: 0.8163, Val Correlation: -0.1203\n",
      "Epoch [560/1000], Loss: -0.8203, Val Loss: 0.1203, Train Correlation: 0.8203, Val Correlation: -0.1203\n",
      "Epoch [570/1000], Loss: -0.8243, Val Loss: 0.1205, Train Correlation: 0.8243, Val Correlation: -0.1205\n",
      "Epoch [580/1000], Loss: -0.8281, Val Loss: 0.1200, Train Correlation: 0.8281, Val Correlation: -0.1200\n",
      "Epoch [590/1000], Loss: -0.8318, Val Loss: 0.1197, Train Correlation: 0.8318, Val Correlation: -0.1197\n",
      "Epoch [600/1000], Loss: -0.8354, Val Loss: 0.1194, Train Correlation: 0.8354, Val Correlation: -0.1194\n",
      "Epoch [610/1000], Loss: -0.8387, Val Loss: 0.1191, Train Correlation: 0.8387, Val Correlation: -0.1191\n",
      "Epoch [620/1000], Loss: -0.8420, Val Loss: 0.1191, Train Correlation: 0.8420, Val Correlation: -0.1191\n",
      "Epoch [630/1000], Loss: -0.8451, Val Loss: 0.1196, Train Correlation: 0.8451, Val Correlation: -0.1196\n",
      "Epoch [640/1000], Loss: -0.8482, Val Loss: 0.1199, Train Correlation: 0.8482, Val Correlation: -0.1199\n",
      "Epoch [650/1000], Loss: -0.8512, Val Loss: 0.1202, Train Correlation: 0.8512, Val Correlation: -0.1202\n",
      "Epoch [660/1000], Loss: -0.8540, Val Loss: 0.1201, Train Correlation: 0.8540, Val Correlation: -0.1201\n",
      "Epoch [670/1000], Loss: -0.8568, Val Loss: 0.1195, Train Correlation: 0.8568, Val Correlation: -0.1195\n",
      "Epoch [680/1000], Loss: -0.8597, Val Loss: 0.1190, Train Correlation: 0.8597, Val Correlation: -0.1190\n",
      "Epoch [690/1000], Loss: -0.8624, Val Loss: 0.1181, Train Correlation: 0.8624, Val Correlation: -0.1181\n",
      "Epoch [700/1000], Loss: -0.8651, Val Loss: 0.1184, Train Correlation: 0.8651, Val Correlation: -0.1184\n",
      "Epoch [710/1000], Loss: -0.8675, Val Loss: 0.1186, Train Correlation: 0.8675, Val Correlation: -0.1186\n",
      "Epoch [720/1000], Loss: -0.8699, Val Loss: 0.1191, Train Correlation: 0.8699, Val Correlation: -0.1191\n",
      "Epoch [730/1000], Loss: -0.8721, Val Loss: 0.1198, Train Correlation: 0.8721, Val Correlation: -0.1198\n",
      "Epoch [740/1000], Loss: -0.8743, Val Loss: 0.1202, Train Correlation: 0.8743, Val Correlation: -0.1202\n",
      "Epoch [750/1000], Loss: -0.8764, Val Loss: 0.1206, Train Correlation: 0.8764, Val Correlation: -0.1206\n",
      "Epoch [760/1000], Loss: -0.8785, Val Loss: 0.1205, Train Correlation: 0.8785, Val Correlation: -0.1205\n",
      "Epoch [770/1000], Loss: -0.8805, Val Loss: 0.1205, Train Correlation: 0.8805, Val Correlation: -0.1205\n",
      "Epoch [780/1000], Loss: -0.8824, Val Loss: 0.1204, Train Correlation: 0.8824, Val Correlation: -0.1204\n",
      "Epoch [790/1000], Loss: -0.8843, Val Loss: 0.1204, Train Correlation: 0.8843, Val Correlation: -0.1204\n",
      "Epoch [800/1000], Loss: -0.8864, Val Loss: 0.1202, Train Correlation: 0.8864, Val Correlation: -0.1202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [810/1000], Loss: -0.8885, Val Loss: 0.1202, Train Correlation: 0.8885, Val Correlation: -0.1202\n",
      "Epoch [820/1000], Loss: -0.8902, Val Loss: 0.1207, Train Correlation: 0.8902, Val Correlation: -0.1207\n",
      "Epoch [830/1000], Loss: -0.8920, Val Loss: 0.1209, Train Correlation: 0.8920, Val Correlation: -0.1209\n",
      "Epoch [840/1000], Loss: -0.8938, Val Loss: 0.1208, Train Correlation: 0.8938, Val Correlation: -0.1208\n",
      "Epoch [850/1000], Loss: -0.8956, Val Loss: 0.1209, Train Correlation: 0.8956, Val Correlation: -0.1209\n",
      "Epoch [860/1000], Loss: -0.8972, Val Loss: 0.1208, Train Correlation: 0.8972, Val Correlation: -0.1208\n",
      "Epoch [870/1000], Loss: -0.8988, Val Loss: 0.1209, Train Correlation: 0.8988, Val Correlation: -0.1209\n",
      "Epoch [880/1000], Loss: -0.9002, Val Loss: 0.1210, Train Correlation: 0.9002, Val Correlation: -0.1210\n",
      "Epoch [890/1000], Loss: -0.9018, Val Loss: 0.1212, Train Correlation: 0.9018, Val Correlation: -0.1212\n",
      "Epoch [900/1000], Loss: -0.9032, Val Loss: 0.1211, Train Correlation: 0.9032, Val Correlation: -0.1211\n",
      "Epoch [910/1000], Loss: -0.9047, Val Loss: 0.1213, Train Correlation: 0.9047, Val Correlation: -0.1213\n",
      "Epoch [920/1000], Loss: -0.9060, Val Loss: 0.1216, Train Correlation: 0.9060, Val Correlation: -0.1216\n",
      "Epoch [930/1000], Loss: -0.9073, Val Loss: 0.1215, Train Correlation: 0.9073, Val Correlation: -0.1215\n",
      "Epoch [940/1000], Loss: -0.9086, Val Loss: 0.1217, Train Correlation: 0.9086, Val Correlation: -0.1217\n",
      "Epoch [950/1000], Loss: -0.9097, Val Loss: 0.1221, Train Correlation: 0.9097, Val Correlation: -0.1221\n",
      "Epoch [960/1000], Loss: -0.9108, Val Loss: 0.1222, Train Correlation: 0.9108, Val Correlation: -0.1222\n",
      "Epoch [970/1000], Loss: -0.9123, Val Loss: 0.1226, Train Correlation: 0.9123, Val Correlation: -0.1226\n",
      "Epoch [980/1000], Loss: -0.9134, Val Loss: 0.1225, Train Correlation: 0.9134, Val Correlation: -0.1225\n",
      "Epoch [990/1000], Loss: -0.9147, Val Loss: 0.1229, Train Correlation: 0.9147, Val Correlation: -0.1229\n",
      "Epoch [1000/1000], Loss: -0.9157, Val Loss: 0.1231, Train Correlation: 0.9157, Val Correlation: -0.1231\n",
      "Test Loss (Negative Correlation): -0.066290944814682\n",
      "Correlation between y_test and y_pred: 0.06629095611893372\n"
     ]
    }
   ],
   "source": [
    "model = BPNeuralNetwork()\n",
    "\n",
    "def negative_correlation_loss(output, target):\n",
    "\n",
    "    output = output - torch.mean(output) \n",
    "    target = target - torch.mean(target)\n",
    "    numerator = torch.sum(output * target) \n",
    "    denominator = torch.sqrt(torch.sum(output ** 2)) * torch.sqrt(torch.sum(target ** 2))  \n",
    "    correlation = numerator / (denominator + 1e-8)  \n",
    "    return -correlation \n",
    "\n",
    "criterion = negative_correlation_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "patience = 10000  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    outputs = model(original_X_train_tensor).flatten()\n",
    "    loss = criterion(outputs, original_y_train_tensor.flatten())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(original_X_val_tensor).flatten()\n",
    "        val_loss = criterion(val_outputs, y_val_tensor.flatten())\n",
    "\n",
    "    # Calculate correlation for training\n",
    "    train_correlation = np.corrcoef(\n",
    "        original_y_train_tensor.numpy().flatten(), outputs.detach().numpy().flatten()\n",
    "    )[0, 1]\n",
    "\n",
    "    # Calculate correlation for validation\n",
    "    val_correlation = np.corrcoef(\n",
    "        original_y_val_tensor.numpy().flatten(), val_outputs.numpy().flatten()\n",
    "    )[0, 1]\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss <= best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    # Print metrics every 10 epochs or at the last epoch\n",
    "    if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "            f\"Train Correlation: {train_correlation:.4f}, Val Correlation: {val_correlation:.4f}\"\n",
    "        )\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(original_X_test_tensor).flatten()\n",
    "    test_loss = criterion(test_outputs, original_y_test_tensor.flatten()).item()\n",
    "\n",
    "    y_pred = test_outputs.numpy()\n",
    "    y_test_array = original_y_test_tensor.numpy()\n",
    "\n",
    "    test_correlation = np.corrcoef(y_test_array.flatten(), y_pred.flatten())[0, 1]\n",
    "\n",
    "    print(f\"Test Loss (Negative Correlation): {test_loss}\")\n",
    "    print(f\"Correlation between y_test and y_pred: {test_correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be31ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
